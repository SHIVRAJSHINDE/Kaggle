{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":275896,"sourceType":"datasetVersion","datasetId":115342}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-20T13:27:03.631443Z","iopub.execute_input":"2024-08-20T13:27:03.631921Z","iopub.status.idle":"2024-08-20T13:27:03.652552Z","shell.execute_reply.started":"2024-08-20T13:27:03.631885Z","shell.execute_reply":"2024-08-20T13:27:03.649525Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stdout","text":"/kaggle/input/spamham/spam.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df=pd.read_csv(\"../input/spamham/spam.csv\",encoding='Windows-1252')\ndf.info()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:27:03.655830Z","iopub.execute_input":"2024-08-20T13:27:03.656673Z","iopub.status.idle":"2024-08-20T13:27:03.707734Z","shell.execute_reply.started":"2024-08-20T13:27:03.656637Z","shell.execute_reply":"2024-08-20T13:27:03.706049Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5572 entries, 0 to 5571\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   v1          5572 non-null   object\n 1   v2          5572 non-null   object\n 2   Unnamed: 2  50 non-null     object\n 3   Unnamed: 3  12 non-null     object\n 4   Unnamed: 4  6 non-null      object\ndtypes: object(5)\nmemory usage: 217.8+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndf = df[['v1','v2']]\ndf.head()\n\nnameDict = {\"v1\":\"label\",\"v2\":\"message\"}\nnameDict\ndf = df.rename(columns=nameDict)\ndf\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:27:03.709698Z","iopub.execute_input":"2024-08-20T13:27:03.710141Z","iopub.status.idle":"2024-08-20T13:27:03.732878Z","shell.execute_reply.started":"2024-08-20T13:27:03.710108Z","shell.execute_reply":"2024-08-20T13:27:03.731480Z"},"trusted":true},"execution_count":109,"outputs":[{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"     label                                            message\n0      ham  Go until jurong point, crazy.. Available only ...\n1      ham                      Ok lar... Joking wif u oni...\n2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3      ham  U dun say so early hor... U c already then say...\n4      ham  Nah I don't think he goes to usf, he lives aro...\n...    ...                                                ...\n5567  spam  This is the 2nd time we have tried 2 contact u...\n5568   ham              Will Ì_ b going to esplanade fr home?\n5569   ham  Pity, * was in mood for that. So...any other s...\n5570   ham  The guy did some bitching but I acted like i'd...\n5571   ham                         Rofl. Its true to its name\n\n[5572 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5567</th>\n      <td>spam</td>\n      <td>This is the 2nd time we have tried 2 contact u...</td>\n    </tr>\n    <tr>\n      <th>5568</th>\n      <td>ham</td>\n      <td>Will Ì_ b going to esplanade fr home?</td>\n    </tr>\n    <tr>\n      <th>5569</th>\n      <td>ham</td>\n      <td>Pity, * was in mood for that. So...any other s...</td>\n    </tr>\n    <tr>\n      <th>5570</th>\n      <td>ham</td>\n      <td>The guy did some bitching but I acted like i'd...</td>\n    </tr>\n    <tr>\n      <th>5571</th>\n      <td>ham</td>\n      <td>Rofl. Its true to its name</td>\n    </tr>\n  </tbody>\n</table>\n<p>5572 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X=list(df['message'])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:27:03.734832Z","iopub.execute_input":"2024-08-20T13:27:03.735487Z","iopub.status.idle":"2024-08-20T13:27:03.745921Z","shell.execute_reply.started":"2024-08-20T13:27:03.735445Z","shell.execute_reply":"2024-08-20T13:27:03.744645Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"y=list(df['label'])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:27:03.749074Z","iopub.execute_input":"2024-08-20T13:27:03.749883Z","iopub.status.idle":"2024-08-20T13:27:03.759574Z","shell.execute_reply.started":"2024-08-20T13:27:03.749849Z","shell.execute_reply":"2024-08-20T13:27:03.758258Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"dictOne = {\"spam\":1,\"ham\":0}\ny = list(df['label'].replace(dictOne))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:27:03.761523Z","iopub.execute_input":"2024-08-20T13:27:03.762083Z","iopub.status.idle":"2024-08-20T13:27:03.779368Z","shell.execute_reply.started":"2024-08-20T13:27:03.762047Z","shell.execute_reply":"2024-08-20T13:27:03.777793Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/1466391086.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  y = list(df['label'].replace(dictOne))\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:27:03.780910Z","iopub.execute_input":"2024-08-20T13:27:03.781343Z","iopub.status.idle":"2024-08-20T13:27:03.795024Z","shell.execute_reply.started":"2024-08-20T13:27:03.781269Z","shell.execute_reply":"2024-08-20T13:27:03.793730Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"# !pip install transformers\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:27:03.796456Z","iopub.execute_input":"2024-08-20T13:27:03.796800Z","iopub.status.idle":"2024-08-20T13:27:21.126982Z","shell.execute_reply.started":"2024-08-20T13:27:03.796772Z","shell.execute_reply":"2024-08-20T13:27:21.125350Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.10/site-packages (from transformers) (0.1.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses->transformers) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses->transformers) (1.4.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\ntokenizer\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:27:21.132082Z","iopub.execute_input":"2024-08-20T13:27:21.132626Z","iopub.status.idle":"2024-08-20T13:27:21.325711Z","shell.execute_reply.started":"2024-08-20T13:27:21.132564Z","shell.execute_reply":"2024-08-20T13:27:21.324506Z"},"trusted":true},"execution_count":115,"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}]},{"cell_type":"code","source":"train_encodings = tokenizer(X_train,truncation = True,padding=True)\ntest_encodings = tokenizer(X_test,truncation=True,padding=True)\ntrain_encodings = dict(train_encodings)\ntest_encodings = dict(test_encodings)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:27:21.327374Z","iopub.execute_input":"2024-08-20T13:27:21.327822Z","iopub.status.idle":"2024-08-20T13:27:22.021396Z","shell.execute_reply.started":"2024-08-20T13:27:21.327781Z","shell.execute_reply":"2024-08-20T13:27:22.020022Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:27:22.023603Z","iopub.execute_input":"2024-08-20T13:27:22.024573Z","iopub.status.idle":"2024-08-20T13:27:22.030144Z","shell.execute_reply.started":"2024-08-20T13:27:22.024525Z","shell.execute_reply":"2024-08-20T13:27:22.028740Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_encodings,y_train))\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_encodings,y_test))\n\ntrain_dataset\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:27:22.031908Z","iopub.execute_input":"2024-08-20T13:27:22.032388Z","iopub.status.idle":"2024-08-20T13:27:30.993692Z","shell.execute_reply.started":"2024-08-20T13:27:22.032345Z","shell.execute_reply":"2024-08-20T13:27:30.992290Z"},"trusted":true},"execution_count":118,"outputs":[{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"<_TensorSliceDataset element_spec=({'input_ids': TensorSpec(shape=(238,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(238,), dtype=tf.int32, name=None)}, TensorSpec(shape=(), dtype=tf.int32, name=None))>"},"metadata":{}}]},{"cell_type":"code","source":"#!pip install transformers==4.18.0\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:27:30.995474Z","iopub.execute_input":"2024-08-20T13:27:30.995829Z","iopub.status.idle":"2024-08-20T13:27:45.763322Z","shell.execute_reply.started":"2024-08-20T13:27:30.995798Z","shell.execute_reply":"2024-08-20T13:27:45.761976Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers==4.18.0 in /opt/conda/lib/python3.10/site-packages (4.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.18.0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.18.0) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.18.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.18.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.18.0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.18.0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.18.0) (2.32.3)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.10/site-packages (from transformers==4.18.0) (0.1.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.18.0) (0.12.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.18.0) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.18.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.18.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.18.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.18.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.18.0) (2024.7.4)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses->transformers==4.18.0) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses->transformers==4.18.0) (1.4.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:27:57.306137Z","iopub.execute_input":"2024-08-20T13:27:57.307332Z","iopub.status.idle":"2024-08-20T13:27:57.312675Z","shell.execute_reply.started":"2024-08-20T13:27:57.307281Z","shell.execute_reply":"2024-08-20T13:27:57.311416Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"from transformers.trainer_tf import TFTrainer\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:28:12.681214Z","iopub.execute_input":"2024-08-20T13:28:12.681786Z","iopub.status.idle":"2024-08-20T13:28:12.688041Z","shell.execute_reply.started":"2024-08-20T13:28:12.681737Z","shell.execute_reply":"2024-08-20T13:28:12.686482Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"from transformers import TFDistilBertForSequenceClassification, TFTrainingArguments\n\ntraining_args = TFTrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=2,              # total number of training epochs\n    per_device_train_batch_size=8,  # batch size per device during training\n    per_device_eval_batch_size=16,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=10,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:28:20.955315Z","iopub.execute_input":"2024-08-20T13:28:20.956332Z","iopub.status.idle":"2024-08-20T13:28:20.966090Z","shell.execute_reply.started":"2024-08-20T13:28:20.956286Z","shell.execute_reply":"2024-08-20T13:28:20.964784Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"with training_args.strategy.scope():\n    model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n\ntrainer = TFTrainer(\n    model=model,                         # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=test_dataset             # evaluation dataset\n)\n\ntrainer.train()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:28:24.649538Z","iopub.execute_input":"2024-08-20T13:28:24.651042Z","iopub.status.idle":"2024-08-20T13:39:43.155474Z","shell.execute_reply.started":"2024-08-20T13:28:24.650986Z","shell.execute_reply":"2024-08-20T13:39:43.153367Z"},"trusted":true},"execution_count":126,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/trainer_tf.py:108: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  \n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  \n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  \n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  \n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  \n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  \n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  \n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  \n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"code","source":"\ntrainer.evaluate(test_dataset)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:39:43.159555Z","iopub.status.idle":"2024-08-20T13:39:43.160134Z","shell.execute_reply.started":"2024-08-20T13:39:43.159836Z","shell.execute_reply":"2024-08-20T13:39:43.159861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}